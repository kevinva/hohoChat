{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c86cfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\AI\\hohoChat\\langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "cd ../langchain-ChatGLM-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc46db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "from configs import model_config\n",
    "from models.chatglm_llm import ChatGLM\n",
    "from chains.local_doc_qa import *\n",
    "from textsplitter import ChineseTextSplitter\n",
    "from utils import torch_gc\n",
    "\n",
    "from langchain.text_splitter import MarkdownTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66df174",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"H:\\AI\\data\\法律\\法律法规文件\\Laws-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c2001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str_YmdHmS():\n",
    "    current_time = time.time()\n",
    "    local_time = time.localtime(current_time)\n",
    "    time_str = time.strftime('%Y%m%d%H%m%S', local_time)\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c9876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths_at_path(item_path):\n",
    "    if os.path.isfile(item_path):\n",
    "        return [item_path]\n",
    "    \n",
    "    result_list = []\n",
    "    for item in os.listdir(item_path):\n",
    "        path = os.path.join(item_path, item)\n",
    "        file_paths = get_filepaths_at_path(path)\n",
    "        result_list.extend(file_paths)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d89b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = get_filepaths_at_path(DATA_DIR)\n",
    "file_paths = [file_path for file_path in file_paths if os.path.basename(file_path) != '_index.md']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a408ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3084"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8851f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = ChineseTextSplitter(pdf = False)\n",
    "text_splitter = MarkdownTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "\n",
    "docs = []\n",
    "for file_path in file_paths:\n",
    "    loader = UnstructuredMarkdownLoader(file_path)\n",
    "    docs += loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc8fac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee8c04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Administrator/.cache\\torch\\sentence_transformers\\nghuyong_ernie-3.0-base-zh. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\Administrator/.cache\\torch\\sentence_transformers\\nghuyong_ernie-3.0-base-zh were not used when initializing ErnieModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = model_config.embedding_model_dict[\"ernie-base\"], \n",
    "                                   model_kwargs = {'device': model_config.EMBEDDING_DEVICE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69acb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c22b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_path = f\"../outputs/vector_store/law_FAISS_{time_str_YmdHmS()}\"\n",
    "vector_store.save_local(vs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4786a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS.similarity_search_with_score_by_vector = similarity_search_with_score_by_vector\n",
    "vector_store.chunk_size = model_config.CHUNK_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c8374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_chatgpt\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.c -shared -o C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.c -shared -o C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\02a065cf2797029c036a02cac30f1da1a9bc49a3\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGLM()\n",
    "llm.load_model(model_name_or_path = model_config.llm_model_dict[model_config.LLM_MODEL],\n",
    "               llm_device = model_config.LLM_DEVICE,\n",
    "               use_ptuning_v2 = model_config.USE_PTUNING_V2)\n",
    "llm.history_len = LLM_HISTORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae9bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_based_on_knowledge(query, vector_store, llm, history = []):\n",
    "    related_docs_with_score = vector_store.similarity_search_with_score(query, k = VECTOR_SEARCH_TOP_K)\n",
    "    related_docs = get_docs_with_score(related_docs_with_score)\n",
    "    prompt = generate_prompt(related_docs, query)\n",
    "\n",
    "    # if streaming:\n",
    "    #     for result, history in self.llm._stream_call(prompt = prompt,history = chat_history):\n",
    "    #         history[-1][0] = query\n",
    "    #         response = {\"query\": query,\n",
    "    #                     \"result\": result,\n",
    "    #                     \"source_documents\": related_docs}\n",
    "    #         yield response, history\n",
    "    # else:\n",
    "    for result, history in llm._call(prompt = prompt, history = history, streaming = False):\n",
    "        history[-1][0] = query\n",
    "        response = {\"query\": query,\n",
    "                    \"result\": result,\n",
    "                    \"source_documents\": related_docs}\n",
    "        yield response, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61328a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def display_answer(query, history = []):\n",
    "    for resp, history in answer_based_on_knowledge(query, vector_store, llm, history):\n",
    "        clear_output(wait = True)\n",
    "        display(Markdown(resp['result']))\n",
    "    \n",
    "    return resp, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f78f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "根据已知信息，信用卡欠款不还可能会遭到以下处罚：\n",
       "\n",
       "1. 利率优惠和信用额度降低：如果信用卡欠款不还，银行可能会降低信用卡的利率，并减少信用额度，以提醒用户及时还款。\n",
       "\n",
       "2. 法律处罚：根据刑法，信用卡欠款不还可能会被视为犯罪行为，会受到法律处罚。例如，可能会被指控信用卡欺诈罪或欠款罪，并面临刑事指控和法律诉讼。\n",
       "\n",
       "3. 信用受损：信用卡欠款不还可能会影响用户的信用，导致用户的信用评分下降，影响其申请贷款和其他金融服务。\n",
       "\n",
       "4. 银行起诉：如果信用卡欠款不还，银行可能会向法院起诉用户，要求其还款。如果用户无法还款，银行可能会申请法院执行，将用户的财产划拨到银行，以完成还款。\n",
       "\n",
       "信用卡欠款不还可能会面临多种处罚，包括利率优惠和信用额度降低、法律处罚、信用受损和银行起诉。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"信用卡欠款不还会遭到什么处罚？\"\n",
    "answer, history = display_answer(question, history = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f8a6f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "根据已知信息，无法回答该问题。没有提供足够的相关信息，无法判断公司是否采取了必要的措施预防和制止性骚扰行为，因此无法判断公司是否需要承担责任。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"一名员工对女同事实施了性骚扰行为，女同事向公司进行举报，但公司却没有采取必要的措施来制止这种行为。\\n\\n公司未采取必要措施预防和制止性骚扰，导致女同事的权益受到侵害，该公司是否需要承担责任？\"\n",
    "answer, history = display_answer(question, history = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "447916fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "根据已知信息，可以回答该问题。\n",
       "\n",
       "强制拆迁可能会对被拆迁人的生活造成严重的影响，因此，如何用法律保护自己是一个关键问题。\n",
       "\n",
       "根据《中华人民共和国宪法》和《中华人民共和国土地管理法》的规定，被拆迁人有权依法维护自己的权益。在遭遇强制拆迁时，被拆迁人可以通过以下方式保护自己：\n",
       "\n",
       "1. 申请法律援助：被拆迁人可以向当地法律援助中心申请法律援助，获得法律咨询和代理服务，帮助自己依法维权。\n",
       "\n",
       "2. 依法维权：被拆迁人可以通过法律途径维护自己的权益，例如向当地法院提起诉讼、申请仲裁等。在诉讼中，被拆迁人需要提供证据，证明房屋属于自己的财产，并依法维护自己的权益。\n",
       "\n",
       "3. 申请补偿：被拆迁人需要依法申请补偿，并了解当地的补偿标准。如果补偿标准不合理，被拆迁人可以通过法律途径维护自己的权益。\n",
       "\n",
       "4. 与拆迁部门沟通：被拆迁人需要与拆迁部门进行沟通，了解拆迁计划和拆迁程序，并协商解决相关问题。如果被拆迁人不同意拆迁计划，可以通过法律途径维护自己的权益。\n",
       "\n",
       "5. 寻求社会支持：被拆迁人可以通过社会支持，获得社会舆论的支持，帮助自己依法维权。\n",
       "\n",
       "在遭遇强制拆迁时，被拆迁人需要依法维护自己的权益，通过法律途径解决问题。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"遭遇强制拆迁 如何用法律保护自己?\"\n",
    "answer, history = display_answer(question, history = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c2bf915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "根据已知信息，可以回答该问题。口头录取通知的法律效力取决于具体的语境和录取程序。在某些情况下，口头录取通知可以具有法律效力，但在其他情况下可能不起作用。例如，在某些学校或教育机构中，可能需要通过书面文件或电子邮件确认录取通知的效力。因此，需要根据具体情况来确定口头录取通知的法律效力。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"口头录取通知有没有法律效力？\"\n",
    "answer, history = display_answer(question, history = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1165d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading seconds: 0.5334064960479736 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "vs_path = \"../outputs/vector_store/law_FAISS_20230518230554\"\n",
    "vector_store2 = FAISS.load_local(vs_path, embeddings)\n",
    "\n",
    "print(f\"loading seconds: {time.time() - start_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd7402b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
