{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b0df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 使用Unstructured File读取text文件、powerpoints、html、PDF、images或其他需要安装以下\n",
    "## windows安装detectron诸多问题，可参考：\n",
    "#####  - https://blog.csdn.net/u010674979/article/details/125719919\n",
    "#####  - http://www.taodudu.cc/news/show-1215672.html\n",
    "\n",
    "# # Install package\n",
    "# !pip install \"unstructured[local-inference]\"\n",
    "# !pip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\"\n",
    "# !pip install layoutparser[layoutmodels,tesseract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2f1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# # Install other dependencies\n",
    "# # https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst\n",
    "# !brew install libmagic\n",
    "# !brew install poppler\n",
    "# !brew install tesseract\n",
    "# # If parsing xml / html documents:\n",
    "# !brew install libxml2\n",
    "# !brew install libxslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232034e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/f/AI/hohoChat\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14c4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\AI\\hohoChat\\langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "cd langchain-ChatGLM-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28fb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897fec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r nltk_data ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74bc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def display_answer(agent, query, vs_path, history = []):\n",
    "    for resp, history in local_doc.qa.get_knowledge_based_answer(query = query,\n",
    "                                                                  vs_path = vs_path,\n",
    "                                                                  chat_history = history,\n",
    "                                                                  streaming = True):\n",
    "        clear_output(wait = True)\n",
    "        display(Markdown(resp['result']))\n",
    "    return resp, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e17e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "from configs import model_config\n",
    "\n",
    "model_config.embedding_model_dict = {\n",
    "    \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n",
    "    \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n",
    "    \"text2vec\": \"/home/mw/input/text2vec2538\",   # hoho: 注意路径\n",
    "}\n",
    "\n",
    "model_config.llm_model_dict = {\n",
    "    \"chatyuan\": \"ClueAI/ChatYuan-large-v2\",\n",
    "    \"chatglm-6b-int4-qe\": \"THUDM/chatglm-6b-int4-qe\",\n",
    "    \"chatglm-6b-int4\": \"THUDM/chatglm-6b-int4\",\n",
    "    \"chatglm-6b-int8\": \"THUDM/chatglm-6b-int8\",\n",
    "    \"chatglm-6b\": \"/home/mw/input/ChatGLM6B6449\",  # hoho: 注意路径\n",
    "}\n",
    "\n",
    "model_config.VS_ROOT_PATH = \"../../temp\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81988c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels_parallel.c -shared -o C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels.c -shared -o C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from chains.local_doc_qa import LocalDocQA\n",
    "\n",
    "EMBEDDING_MODEL = \"text2vec-base\"\n",
    "VECTOR_SEARCH_TOP_K = 6\n",
    "LLM_MODEL = \"chatglm-6b-int4\"\n",
    "LLM_HISTORY_LEN = 3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "local_doc_qa = LocalDocQA()\n",
    "\n",
    "local_doc_qa.init_cfg(llm_model = LLM_MODEL,\n",
    "                     embedding_model = EMBEDDING_MODEL,\n",
    "                     llm_history_len = LLM_HISTORY_LEN,\n",
    "                     top_k = VECTOR_SEARCH_TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20157502",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_path, _ = local_doc_qa.init_knowledge_vector_store(\"F:/AI/hohoChat/data/d2l-zh-pytorch.pdf\")\n",
    "vs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e60e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/f/AI/hohoChat/langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37cdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
