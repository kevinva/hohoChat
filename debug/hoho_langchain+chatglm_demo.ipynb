{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b0df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 使用Unstructured File读取text文件、powerpoints、html、PDF、images或其他需要安装以下\n",
    "## windows安装detectron诸多问题，可参考：\n",
    "#####  - https://blog.csdn.net/u010674979/article/details/125719919\n",
    "#####  - http://www.taodudu.cc/news/show-1215672.html\n",
    "\n",
    "# # Install package\n",
    "# !pip install \"unstructured[local-inference]\"\n",
    "# !pip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\"\n",
    "# !pip install layoutparser[layoutmodels,tesseract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2f1bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'brew' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# # Install other dependencies\n",
    "# # https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst\n",
    "# !brew install libmagic\n",
    "# !brew install poppler\n",
    "# !brew install tesseract\n",
    "# # If parsing xml / html documents:\n",
    "# !brew install libxml2\n",
    "# !brew install libxslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232034e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/f/AI/hohoChat\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14c4513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\AI\\hohoChat\\langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "cd langchain-ChatGLM-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28fb5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897fec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r nltk_data ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74bc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def display_answer(agent, query, vs_path, history = []):\n",
    "    for resp, history in local_doc.qa.get_knowledge_based_answer(query = query,\n",
    "                                                                  vs_path = vs_path,\n",
    "                                                                  chat_history = history,\n",
    "                                                                  streaming = True):\n",
    "        clear_output(wait = True)\n",
    "        display(Markdown(resp['result']))\n",
    "    return resp, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e17e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "from configs import model_config\n",
    "\n",
    "model_config.embedding_model_dict = {\n",
    "    \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n",
    "    \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n",
    "    \"text2vec\": \"/home/mw/input/text2vec2538\",   # hoho: 注意路径\n",
    "}\n",
    "\n",
    "model_config.llm_model_dict = {\n",
    "    \"chatyuan\": \"ClueAI/ChatYuan-large-v2\",\n",
    "    \"chatglm-6b-int4-qe\": \"THUDM/chatglm-6b-int4-qe\",\n",
    "    \"chatglm-6b-int4\": \"THUDM/chatglm-6b-int4\",\n",
    "    \"chatglm-6b-int8\": \"THUDM/chatglm-6b-int8\",\n",
    "    \"chatglm-6b\": \"/home/mw/input/ChatGLM6B6449\",  # hoho: 注意路径\n",
    "}\n",
    "\n",
    "model_config.VS_ROOT_PATH = \"../../temp\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81988c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels_parallel.c -shared -o C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels_parallel.so\n",
      "Compile default cpu kernel failed, using default cpu kernel code.\n",
      "Compiling gcc -O3 -fPIC -std=c99 C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels.c -shared -o C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm-6b-int4\\f6b88da8c13be209fdaa3bfe2d3099563947a0ca\\quantization_kernels.so\n",
      "Compile default cpu kernel failed.\n",
      "Failed to load kernel.\n",
      "Cannot load cpu kernel, don't use quantized model on cpu.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "from chains.local_doc_qa import LocalDocQA\n",
    "\n",
    "EMBEDDING_MODEL = \"text2vec-base\"\n",
    "VECTOR_SEARCH_TOP_K = 6\n",
    "LLM_MODEL = \"chatglm-6b-int4\"\n",
    "LLM_HISTORY_LEN = 3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "local_doc_qa = LocalDocQA()\n",
    "\n",
    "local_doc_qa.init_cfg(llm_model = LLM_MODEL,\n",
    "                     embedding_model = EMBEDDING_MODEL,\n",
    "                     llm_history_len = LLM_HISTORY_LEN,\n",
    "                     top_k = VECTOR_SEARCH_TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20157502",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_path, _ = local_doc_qa.init_knowledge_vector_store(\"F:/AI/hohoChat/data/d2l-zh-pytorch.pdf\")\n",
    "vs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e60e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/f/AI/hohoChat/langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37cdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c229c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92aec210",
   "metadata": {},
   "source": [
    "### 自定义数据源读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e18e772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/dev_cu/hohoChat/debug\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05bb53da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev_cu\\hohoChat\\langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "cd ../langchain-ChatGLM-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ad85f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/dev_cu/hohoChat/langchain-ChatGLM-master\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d605663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "from configs import model_config\n",
    "from models.chatglm_llm import ChatGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cc2e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4118541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.embedding_model_dict = {\n",
    "    \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n",
    "    \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n",
    "}\n",
    "\n",
    "model_config.llm_model_dict = {\n",
    "    \"chatyuan\": \"ClueAI/ChatYuan-large-v2\",\n",
    "    \"chatglm-6b-int4-qe\": \"THUDM/chatglm-6b-int4-qe\",\n",
    "    \"chatglm-6b-int4\": \"THUDM/chatglm-6b-int4\",\n",
    "    \"chatglm-6b-int8\": \"THUDM/chatglm-6b-int8\",\n",
    "}\n",
    "\n",
    "EMBEDDING_MODEL = \"text2vec-base\"\n",
    "VECTOR_SEARCH_TOP_K = 6\n",
    "LLM_MODEL = \"chatglm-6b-int4\"\n",
    "LLM_HISTORY_LEN = 3\n",
    "LLM_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c22a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('../data/2023_GPT4All-J_Technical_Report_2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c7df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52152251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbau'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371e7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db32c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00bee8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzach@nomic.aiBrandon Duderstadt\\nbrandon@nomic.ai\\nBenjamin M. Schmidt\\nben@nomic.aiAdam Treat\\ntreat.adam@gmail.comAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nGPT4All-J is an Apache-2 licensed chatbot\\ntrained over a massive curated corpus of as-\\nsistant interactions including word problems,\\nmulti-turn dialogue, code, poems, songs, and\\nstories. It builds on the March 2023 GPT4All\\nrelease by training on a significantly larger\\ncorpus, by deriving its weights from the\\nApache-licensed GPT-J model rather than the\\nGPL-licensed of LLaMA, and by demonstrat-\\ning improved performance on creative tasks\\nsuch as writing stories, poems, songs and\\nplays. We openly release the training data,\\ndata curation procedure, training code, and fi-\\nnal model weights to promote open research\\nand reproducibility. Additionally, we release\\nPython bindings and a Chat UI to a quantized'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86e7964b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data curation procedure, training code, and fi-\\nnal model weights to promote open research\\nand reproducibility. Additionally, we release\\nPython bindings and a Chat UI to a quantized\\n4-bit version of GPT4All-J allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe gather a diverse sample of questions/prompts\\nby leveraging several publicly available datasets\\nand curating our own set of prompts:\\n• Several subsamples from subsets of\\nLAION OIG including unified chip2,\\nunified unifiedskg instruction, uni-\\nfiedhc3human, unified multi news and\\nunified abstract infill\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\n• Custom-generated creative questions.\\nWe accompany this paper with the 800k point\\nGPT4All-J dataset that is a superset of the origi-\\nnal 400k points GPT4All dataset. We dedicated\\nsubstantial attention to data preparation and cura-'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ff0d496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec0126be57046efaeae2b3747c66929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)86555/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d446accc33974ece8fd43b51147b4b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac1c06ef6884d43ab04fea7881e112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)75eb086555/README.md:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4868d1fbd3f543cbbda419b0b04efafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)eb086555/config.json:   0%|          | 0.00/856 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdf062fac524206adba6ee8d6165765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b75eb086555/logs.txt:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3de252157e64bc6abcfe463a5c686b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfed8648ff544d65b242213e974f9009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40462b4cc44c4c24aeaa04b239206218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd8859d53d4be69baf9daa02bbf327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c34bcf24de4061ada357add12cf5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)75eb086555/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2132e7213d4ca8b5634f6f08dbe7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b086555/modules.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = model_config.embedding_model_dict[EMBEDDING_MODEL], \n",
    "                                   model_kwargs = {'device': model_config.EMBEDDING_DEVICE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc9765c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts(texts, embeddings)\n",
    "# vector_store = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9fe3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e175a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
